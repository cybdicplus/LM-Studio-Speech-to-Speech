                        ReadMe 

ğŸ™ï¸ CoquiPipeline

A fully local **speech-to-text + AI reply + text-to-speech** system using **Whisper**, **LM Studio**, and **CoquiTTS**.  
No cloud, no API keys â€” 100% offline.

---

ğŸ›  Requirements
<img width="1920" height="1080" alt="logo with name on them 1" src="https://github.com/user-attachments/assets/d6ed8502-fb96-4ed7-ac80-b0864d406397" />
- Python **3.10+**
- Git  
- Conda (recommended)
- LM Studio installed
- OpenAI Whisper
- Coqui TTS
- GPU with CUDA *(optional but much faster)*

âœ¨ Features

- ğŸ’» 100% offline â€” runs locally on your PC  
- ğŸ”’ No API keys or cloud services needed  
- ğŸ§  Works with any model in LM Studio  
- ğŸ§© Modular â€” swap in different STT/TTS engines  
- âš¡ Lightweight and GPU-accelerated (CUDA supported)  
- ğŸ­ Choose voice models, speakers, emotions, and even do **voice cloning**  

---

ğŸ“¦ Setup

1ï¸âƒ£ Clone this repo
git clone https://github.com/cybdicplus/LM-Studio-Speech-to-Speech.git
cd LM-Studio-Speech-to-Speech

2ï¸âƒ£ Install Whisper (Speech-to-Text)
conda create -n whisper python=3.10.11 -y
conda activate whisper
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -U openai-whisper

3ï¸âƒ£ Install CoquiTTS (Text-to-Speech)

1. Create a folder named CoquiTTS.

2. Open it in CMD:

Click the path bar
Delete the path
Type cmd
Press Enter

Then run:
conda create -n coqui python=3.10 -y
conda activate coqui
git clone https://github.com/coqui-ai/TTS.git
cd TTS
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install TTS
pip install sounddevice numpy openai-whisper requests termcolor pyfiglet keyboard

Then create another folder named CoquiPipeline and place inside it:
CoquiPipeline/
â”œâ”€ pipeline_Coqui.py
â””â”€ start_sts.bat

â–¶ï¸ Starting the Pipeline

You can start the voice pipeline in **two ways**:

ğŸ§  1. Run it manually (no `.bat` file)

Open your terminal or Anaconda Prompt and run:
conda activate coqui
cd C:\Users\<YOUR_USERNAME>\CoquiPipeline
python pipeline_Coqui.py
Replace <YOUR_USERNAME> with your Windows username.

âš™ï¸ 2. Create your own start_sts.bat file (optional)
If you want to start it by double-clicking, make your own .bat file:

1. Open Notepad

2. Paste the following:
@echo off
echo ========================================
echo   ğŸš€ Starting Whisper + LM Studio + Coqui TTS Pipeline
echo ========================================

:: Activate your Conda environment
call conda activate coqui

:: Go to your CoquiPipeline folder
cd /d C:\Users\<YOUR_USERNAME>\CoquiPipeline

:: Run the Python script
python pipeline_Coqui.py

pause

3. Replace <YOUR_USERNAME> with your actual username

4. Save the file as: start_sts.bat

5. Double-click it to start your voice pipeline automatically ğŸ¯


To start, just double-click start_sts.bat âœ…
4ï¸âƒ£ LM Studio Setup

1. Download LM Studio â†’ https://lmstudio.ai

2. Install and run a local model (Qwen, Llama, Mistral, etc.)

3. Enable the Local Server API (default:
   http://localhost:1234/v1/chat/completions)

Switching models:
Just switch models inside LM Studio â€” your Python script automatically uses the active one.

Recommended uncensored models:

uncensoredai_UncensoredLM-DeepSeek-R1-Distill-Qwen-14B
Dolphin3.0-Llama3.1-8B


ğŸš€ Usage
<img width="1920" height="1080" alt="pipeline workflow with icons" src="https://github.com/user-attachments/assets/95bc1bce-f3c3-4c0d-b61c-eb8324327112" />
1. Speak into your mic ğŸ¤
2. Whisper â†’ converts speech to text
3. LM Studio â†’ generates AI reply
4. CoquiTTS â†’ speaks the reply back ğŸ”Š

ğŸ”˜ Optional Features:

Type Y for Yes or N for No

Choose by typing the number next to your option

ğŸ™ Choose a Voice Model
| # | Model                                            | Description                                     |
| - | ------------------------------------------------ | ----------------------------------------------- |
| 1 | `tts_models/en/ljspeech/tacotron2-DDC`           | Default female voice                            |
| 2 | `tts_models/en/ljspeech/glow-tts`                | Slightly robotic                                |
| 3 | `tts_models/en/vctk/vits`                        | 109 voices available                            |
| 4 | `tts_models/multilingual/multi-dataset/your_tts` | 6 voices (3 male / 3 female) across 4 languages |


ğŸ­ Voice Emotions
1. Neutral
2. Happy
3. Sad
4. Angry
5. Excited

ğŸ§¬ Voice Cloning
When asked for a .wav file path:

Record or use your 6-second voice sample
Right-click â†’ Copy as path
Paste it when prompted


ğŸ›‘ Stop / Pause Controls
Say â€œstop listeningâ€ â†’ to stop the model
Press P â†’ pause/resume
Press S â†’ stop speaking mid-sentence


ğŸ¬ **Demo Video**

[![Watch the video](https://img.youtube.com/vi/r3UFkIF_Hmw/0.jpg)](https://youtu.be/r3UFkIF_Hmw)



ğŸ”„ Updating
To update to the latest version:
Go to the cybdic GitHub page
Download the new pipeline_Coqui.py
Replace your old one inside CoquiPipeline


ğŸ“¢ Credits to:

- [Python](https://www.python.org/)
- [Git](https://git-scm.com/)
- [Conda](https://docs.conda.io/)
- [OpenAI Whisper](https://github.com/openai/whisper)
- [Coqui TTS](https://github.com/coqui-ai/TTS)
- [LM Studio](https://lmstudio.ai/)
  
Thanks for making this possible ğŸ™ğŸ˜





















                     





